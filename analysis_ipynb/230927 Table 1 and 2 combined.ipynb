{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only permit vertex cover\n",
    "\n",
    "canonical_order = ['BarabasiAlbert', 'ErdosRenyi', 'PowerlawCluster', 'WattsStrogatz',  'MUTAG', 'ENZYMES', 'PROTEINS',   'IMDB-BINARY', 'COLLAB']\n",
    "def reorder(df, canonical_order=canonical_order, by='dataset', extras=['dataset','gen_n', 'gen_n_max'], secondary='gen_n', columns=None):\n",
    "    df['dataset_name_order'] = df[by].map({name: i for i, name in enumerate(canonical_order)})\n",
    "    if secondary is not None:\n",
    "        df = df.sort_values(by=['dataset_name_order', secondary])\n",
    "    else:\n",
    "        df = df.sort_values(by=['dataset_name_order'])\n",
    "\n",
    "    df.drop('dataset_name_order', axis =1, inplace=True)\n",
    "\n",
    "    if columns:\n",
    "        return df[columns ]\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'max_cut'\n",
    "#task = 'vertex_cover'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['legacy',\n",
       " 'Testing',\n",
       " '230928_snapshot',\n",
       " 'LiftMP_runs',\n",
       " '230924_hparam2',\n",
       " '230927_snapshot',\n",
       " '230928_runs',\n",
       " '230924_hparam',\n",
       " '230924_hparam_TU_multiarch',\n",
       " '230924_hparam_TU',\n",
       " '230926_finetune_ER_runs']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('/home/bcjexu/maxcut-80/bespoke-gnn4do/training_runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['230927_snapshot/230925_TUsmall_GAT_VC', '230927_snapshot/230925_TUsmall_GIN_VC', '230927_snapshot/230925_generated_preset_cut', '230927_snapshot/230925_TUlarge_all_cut', '230927_snapshot/230925_TUsmall_GCNN_VC', '230927_snapshot/230925_generated_liftMP_VC', '230927_snapshot/230925_TUlarge_all_VC', '230927_snapshot/230925_generated_preset_VC', '230927_snapshot/230925_TUsmall_liftMP_VC', '230927_snapshot/230925_TUsmall_GatedGCNN_VC', '230927_snapshot/230925_generated_liftMP_cut']\n"
     ]
    }
   ],
   "source": [
    "print(list(os.path.join('230927_snapshot', x) for x in os.listdir('/home/bcjexu/maxcut-80/bespoke-gnn4do/training_runs/230927_snapshot')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_folders = ['LiftMP_runs', '230927_snapshot/230925_TUsmall_GAT_VC', '230927_snapshot/230925_TUsmall_GIN_VC', \n",
    "               '230927_snapshot/230925_generated_preset_cut', '230927_snapshot/230925_TUlarge_all_cut', '230927_snapshot/230925_TUsmall_GCNN_VC',\n",
    "                 '230927_snapshot/230925_generated_liftMP_VC', '230927_snapshot/230925_TUlarge_all_VC', '230927_snapshot/230925_generated_preset_VC', \n",
    "                 '230927_snapshot/230925_TUsmall_liftMP_VC', '230927_snapshot/230925_TUsmall_GatedGCNN_VC', '230927_snapshot/230925_generated_liftMP_cut']\n",
    "\n",
    "run_folders = ['230928_snapshot/230925_TUsmall_GAT_VC', '230928_snapshot/230925_TUsmall_GIN_cut', \n",
    "               '230928_snapshot/230925_generated_preset_cut', '230928_snapshot/230925_TUsmall_GAT_cut', \n",
    "               '230928_snapshot/230925_TUsmall_liftMP_cut', '230928_snapshot/230925_TUsmall_GCNN_VC', \n",
    "               '230928_snapshot/230925_TUsmall_GCNN_cut', '230928_snapshot/230925_generated_liftMP_VC', \n",
    "               '230928_snapshot/230925_generated_preset_VC', '230928_snapshot/230925_TUsmall_liftMP_VC', '230928_snapshot/230925_TUsmall_GatedGCNN_VC', \n",
    "               '230928_snapshot/230925_TUsmall_VC_32', '230928_snapshot/230925_generated_liftMP_cut', '230928_snapshot/230925_TUsmall_GatedGCNN_cut']\n",
    "\n",
    "run_folders = ['230928_runs/230925_TUsmall_GAT_VC', '230928_runs/230925_TUsmall_GIN_cut', \n",
    "               '230928_runs/230925_generated_preset_cut', '230928_runs/230925_TUsmall_GAT_cut', \n",
    "               '230928_runs/230925_TUsmall_liftMP_cut', '230928_runs/230925_TUsmall_GCNN_VC', \n",
    "               '230928_runs/230925_TUsmall_GCNN_cut', '230928_runs/230925_generated_liftMP_VC', '230928_runs/230925_generated_preset_VC', \n",
    "               '230928_runs/230925_TUsmall_liftMP_VC', '230928_runs/230925_TUsmall_GatedGCNN_VC', '230928_runs/230925_TUsmall_VC_32', \n",
    "               '230928_runs/230925_TUlarge_liftMP_cut', '230928_runs/230925_TUlarge_liftMP_VC', '230928_runs/230925_generated_liftMP_cut', '230928_runs/230925_TUsmall_GatedGCNN_cut']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "folder_path = '/home/bcjexu/maxcut-80/bespoke-gnn4do/training_runs'\n",
    "model_list = [os.path.join(folder_path, run_folder, x) for run_folder in run_folders for x in os.listdir(os.path.join(folder_path, run_folder))  ]\n",
    "\n",
    "\n",
    "rows = []\n",
    "\n",
    "errored = []\n",
    "for model_folder in model_list:\n",
    "    try:\n",
    "        with open(os.path.join(model_folder, 'params.txt'), 'r') as f:\n",
    "            model_args = json.load(f)\n",
    "        if model_args['problem_type'] != task:\n",
    "            continue\n",
    "        \n",
    "        losses = np.load(os.path.join(model_folder, 'valid_scores.npy'))\n",
    "        test_losses = np.load(os.path.join(model_folder, 'test_scores.npy'))\n",
    "\n",
    "        modeldict = model_args #{x: model_args[x] for x in params}\n",
    "        modeldict['max_valid_score'] = max(losses)\n",
    "        modeldict['max_valid_epoch'] = np.argmax(losses)\n",
    "        modeldict['scores'] = test_losses[np.argmax(losses)]\n",
    "        modeldict['baseline'] = False\n",
    "\n",
    "\n",
    "        # set the time per pred\n",
    "        time_per_pred = None\n",
    "        scorefile = [x for x in os.listdir(model_folder) if x.startswith('retest_best')]\n",
    "        if len(scorefile) >= 1:\n",
    "            times, _ = np.load(os.path.join(model_folder, scorefile[-1]))\n",
    "            time_per_pred = np.average(times)\n",
    "        modeldict['time_per_pred'] = time_per_pred\n",
    "\n",
    "\n",
    "        # reset test score if the validation is better.\n",
    "        valid_score_from_file = -np.inf\n",
    "        test_score_from_file = -np.inf\n",
    "\n",
    "        for prefix in ['revalidate_best', 'revalidate_last']:\n",
    "            scorefile = [x for x in os.listdir(model_folder) if x.startswith(prefix)]\n",
    "            #assert(len(scorefile) <=1)\n",
    "            if len(scorefile) >= 1:\n",
    "                _, scores = np.load(os.path.join(model_folder, scorefile[-1]))\n",
    "                valid_score = np.average(scores)\n",
    "\n",
    "                if valid_score > valid_score_from_file:\n",
    "                    testscorefile = [x for x in os.listdir(model_folder) if x.startswith(prefix.replace('validate', 'test'))]\n",
    "                    _, test_scores =  np.load(os.path.join(model_folder, testscorefile[0]))\n",
    "                    test_score_from_file = np.average(test_scores)\n",
    "                    valid_score_from_file = valid_score\n",
    "        \n",
    "\n",
    "                        \n",
    "        if valid_score_from_file > modeldict['max_valid_score']:\n",
    "            modeldict['max_valid_score'] = valid_score_from_file\n",
    "            modeldict['scores'] = test_score_from_file\n",
    "        #if model_args['dataset'].startswith('REDDIT'):\n",
    "        #    print(modeldict['max_valid_score'], test_score_from_file)\n",
    "        rows.append(modeldict)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'{e} is wrong w/ {model_folder}')\n",
    "        errored.append(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in baselines\n",
    "if task != 'max_cut':\n",
    "    baseline_file = 'vc_baseline_scores.csv'\n",
    "else:\n",
    "    baseline_file = 'mc_baseline_scores.csv'\n",
    "baselines = pd.read_csv(baseline_file)\n",
    "\n",
    "gen_n_dict = dict(zip([50, 100, 400], [[50, 100], [100, 200],[400, 500]]))\n",
    "\n",
    "# unwind them \n",
    "for i, baseline in baselines.iterrows():\n",
    "    #print(baseline.index)\n",
    "    for col in baselines.keys():\n",
    "        if col == 'Unnamed: 0':\n",
    "            continue\n",
    "        ds = col\n",
    "        gen_n = np.nan\n",
    "        if len(col.split('@@')) > 1:\n",
    "            ds, gen_n = col.split('@@')\n",
    "            gen_n = gen_n_dict[int(gen_n)]\n",
    "\n",
    "        row = {'dataset': ds, 'gen_n': gen_n, 'model_type': baseline['Unnamed: 0'], 'scores': baseline[col], 'baseline': True}\n",
    "        #print(row)\n",
    "        rows.append(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(errored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "Counter(df[df.gen_n == 400].model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for x in df.gen_n:\n",
    "    if not isinstance(x, list) and x !=100:\n",
    "        print(x)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gen_n_list'] = df.gen_n\n",
    "df['gen_n_max'] = df.gen_n_list.apply(lambda x: int(x[1]) if isinstance(x,list) else x)\n",
    "df.gen_n = df.gen_n.apply(lambda x: int(x[0]) if isinstance(x,list) else x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(df.gen_n_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for x, y in zip(df.gen_n, df.gen_n_max):\n",
    "    if x!=y : print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keys()\n",
    "# 'problem_type', 'seed',  'prefix', 'RB_n', 'RB_k', 'log_dir',\n",
    "relevant_keys = [ 'model_type', 'num_layers',\n",
    "       'repeat_lift_layers', 'num_layers_project', 'rank', 'vc_penalty', 'gen_n', 'gen_n_max',\n",
    "       'dataset', 'infinite',  'positional_encoding', 'pe_dimension',\n",
    "       'max_valid_score', 'max_valid_epoch',\n",
    "       'scores']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.histplot(df[df.infinite==False].max_valid_epoch)\n",
    "plt.title('the step (x1000) at which the lowest validation score is achieved (non-infinite)')\n",
    "plt.xlabel(\"step (x1000)\")\n",
    "plt.ylabel('count')\n",
    "plt.show()\n",
    "df[(df.max_valid_epoch > 90) & (df.infinite==False)][relevant_keys] #.to_csv('230925_checkup_late_bloomers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sns.histplot(df[df.infinite!=False].max_valid_epoch, bins=range(20))\n",
    "plt.title('the step (x1000) at which the lowest validation score is achieved (infinite)')\n",
    "plt.xlabel(\"step (x1000)\")\n",
    "plt.ylabel('count')\n",
    "plt.show()\n",
    "df[(df.max_valid_epoch > 18) & (df.infinite!=False)][relevant_keys] #.to_csv('230925_checkup_late_bloomers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.loc[df[df.baseline == False].groupby(['dataset', 'gen_n'])['scores'].idxmax()][relevant_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gen_n = df.gen_n.fillna(\"\")\n",
    "df.gen_n_max = df.gen_n_max.fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test models\n",
    "\n",
    "models_for_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_by_arc = pd.DataFrame()\n",
    "dss = ['BarabasiAlbert', 'ErdosRenyi', 'PowerlawCluster', 'WattsStrogatz']\n",
    "mts = ['SDP proj', 'gurobi_2.0', 'gurobi_4.0', 'gurobi_8.0', 'vertex count']\n",
    "#baseline_names = ['gurobi_2.0', 'gurobi_4.0', 'gurobi_8.0']\n",
    "\n",
    "\n",
    "\n",
    "for (mt, ds, gen_n), group in df[(df.infinite == False) | (df.dataset == 'ErdosRenyi')].groupby(['model_type', 'dataset', 'gen_n']):\n",
    "    if mt in mts:\n",
    "        continue\n",
    "    if all(group['max_valid_score'].isna()):\n",
    "        continue\n",
    "    if ds not in dss:\n",
    "        gen_n = \"\"\n",
    "\n",
    "    #print(mt, ds, gen_n)\n",
    "    bestidx = group[\"max_valid_score\"].idxmax()\n",
    "    best_time = df.loc[bestidx][\"time_per_pred\"]*1000\n",
    "    mult = 1\n",
    "    if task == 'vertex_cover':\n",
    "        mult = -1\n",
    "    best_score = mult*df.loc[bestidx]['scores']\n",
    "    score_time_string = f'{best_score:0.2f} ({best_time:0.0f})'\n",
    "    if ds in dss:\n",
    "        dataset_by_arc.at[f'{ds}, {gen_n}', mt] = score_time_string\n",
    "        dataset_by_arc.at[f'{ds}, {gen_n}', 'gen_n'] = gen_n\n",
    "        dataset_by_arc.at[f'{ds}, {gen_n}', 'dataset'] = ds\n",
    "    else: \n",
    "        dataset_by_arc.at[f'{ds}', mt] = score_time_string\n",
    "        dataset_by_arc.at[f'{ds}', 'gen_n'] = gen_n\n",
    "        dataset_by_arc.at[f'{ds}', 'dataset'] = ds\n",
    "\n",
    "    try:\n",
    "        models_for_test.append((df.loc[bestidx]['log_dir'], df.loc[bestidx]['dataset'], df.loc[bestidx]['gen_n'])) \n",
    "    except Exception as e:\n",
    "        print(f'{e}')\n",
    "\n",
    "#dataset_by_arc['dataset'] = dataset_by_arc.index\n",
    "reorder(dataset_by_arc[[k for k in dataset_by_arc.keys() if k not in ['SDP lift', 'edge count', 'vertex_count']]], by='dataset', columns = ['dataset', 'gen_n', 'GAT', \n",
    "                                                                                                                            'GCNN', 'GIN', 'GatedGCNN', 'LiftMP']).to_csv(f'Table1_{task}.csv', index=False) #.style.highlight_max(color = 'green', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1819292/396912827.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['dataset_name_order'] = df[by].map({name: i for i, name in enumerate(canonical_order)})\n"
     ]
    }
   ],
   "source": [
    "dataset_by_arc = pd.DataFrame()\n",
    "mts = ['GAT', 'GCNN', 'GIN', 'GatedGCNN', 'SDP proj']\n",
    "dss = ['BarabasiAlbert', 'ErdosRenyi', 'PowerlawCluster', 'WattsStrogatz']\n",
    "\n",
    "for (mt, ds, gen_n, gen_nmax), group in df[~df.model_type.isin(mts)].groupby(['model_type', 'dataset', 'gen_n', 'gen_n_max']):\n",
    "\n",
    "    #if mt =='gurobi_4.0': print(ds) # and ds =='MUTAG': print(\"hi\")\n",
    "    #print(mt, ds, gen_n)\n",
    "    if all(group['max_valid_score'].isna()) and mt in ['LiftMP'] + mts:\n",
    "        continue\n",
    "    if ds not in dss:\n",
    "        gen_n = \"\"\n",
    "        gen_nmax = \"\"\n",
    "    #print(mt, ds, gen_n)\n",
    "    if ds in dss:\n",
    "        k = f'{ds}_{gen_n}'\n",
    "    else:\n",
    "        k = ds\n",
    "\n",
    "    mult = 1\n",
    "    if task == 'vertex_cover':\n",
    "        mult = -1\n",
    "\n",
    "    if mt not in  ['LiftMP'] + mts:\n",
    "        assert(len(group) == 1)\n",
    "        dataset_by_arc.at[k, mt] = mult*group['scores'].max()\n",
    "    else:\n",
    "        dataset_by_arc.at[k, 'Type'] = ds\n",
    "        dataset_by_arc.at[k, 'Nmin'] = gen_n\n",
    "        dataset_by_arc.at[k, 'Nmax'] = gen_nmax\n",
    "\n",
    "        bestidx = group[\"max_valid_score\"].idxmax()\n",
    "        best_time = df.loc[bestidx][\"time_per_pred\"]*1000\n",
    "        best_score = mult*df.loc[bestidx]['scores']\n",
    "        score_time_string = f'{best_score:0.2f} ({best_time:0.0f})'\n",
    "        dataset_by_arc.at[k, mt] = score_time_string\n",
    "\n",
    "    try:\n",
    "        models_for_test.append((df.loc[bestidx]['log_dir'], df.loc[bestidx]['dataset'], df.loc[bestidx]['gen_n'])) \n",
    "    except Exception as e:\n",
    "        print(f'{e}')\n",
    "\n",
    "#dataset_by_arc.rename(columns={'Nikos': 'CustomLiftProject'}, inplace=True)\n",
    "reorder(dataset_by_arc[[k for k in dataset_by_arc.keys() if k not in ['SDP lift', 'vertex count', 'edge count']]], by='Type', secondary='Nmin').round(2).to_csv(f'Table2_{task}.csv') #.style.highlight_max(color = 'green', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reorder(dataset_by_arc[[k for k in dataset_by_arc.keys() if k not in ['SDP lift', 'vertex count']]], by='Type', secondary='Nmin').round(2).keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models_for_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df[(df.model_type == 'gurobi_4.0') & (df.dataset == 'ENZYMES')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for x in models_for_test:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maxcut-80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
