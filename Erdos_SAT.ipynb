{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\" #imports\n",
    "#imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "%config Completer.use_jedi = False\n",
    "from torch_scatter import scatter\n",
    "\n",
    "%env CUDA_LAUNCH_BLOCKING 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8054/495279268.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aspect/anaconda3/envs/extensions/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch_geometric\n",
    "import pysat as ps\n",
    "# import gurobipy as gp\n",
    "# from gurobipy import GRB\n",
    "\n",
    "from pysat.formula import CNF\n",
    "from matplotlib import pyplot as plt\n",
    "from pysat.solvers import Lingeling\n",
    "\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "from IPython.core.debugger import set_trace\n",
    "from datetime import date\n",
    "#from kernel.datasets import get_dataset\n",
    "from itertools import product\n",
    "import time\n",
    "from torch import tensor\n",
    "from torch.optim import Adam\n",
    "from torch.optim import SGD\n",
    "from torch.nn import PReLU\n",
    "from torch_geometric.data import DataLoader, DenseDataLoader as DenseLoader\n",
    "from math import ceil\n",
    "from torch.nn import Linear\n",
    "from torch.distributions import categorical\n",
    "from torch.distributions import Bernoulli\n",
    "import torch.nn\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from torch_geometric.utils import convert as cnv\n",
    "from torch_geometric.utils import sparse as sp\n",
    "from torch_geometric.data import Data\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "import networkx as nx\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "from torch.nn.functional import gumbel_softmax\n",
    "from torch.distributions import relaxed_categorical\n",
    "from torch_geometric.nn.inits import uniform\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.nn import GINConv, GATConv, global_mean_pool, NNConv, GCNConv\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU, LeakyReLU\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Sequential, ReLU, BatchNorm1d as BN\n",
    "from torch_geometric.nn import GINConv, global_mean_pool\n",
    "from torch_geometric.data import Batch \n",
    "from torch import autograd\n",
    "from torch_geometric.utils import to_dense_batch, to_dense_adj\n",
    "from torch_geometric.utils import softmax, add_self_loops, remove_self_loops, segregate_self_loops, remove_isolated_nodes, contains_isolated_nodes, add_remaining_self_loops\n",
    "from torch_geometric.utils import dropout_adj, to_undirected, to_networkx\n",
    "from torch_geometric.utils import is_undirected\n",
    "import scipy\n",
    "import scipy.io\n",
    "from matplotlib.lines import Line2D\n",
    "from torch_geometric.utils.convert import from_scipy_sparse_matrix\n",
    "from networkx.algorithms.approximation import max_clique\n",
    "import pickle\n",
    "from torch_geometric.nn import SplineConv, global_mean_pool, DataParallel\n",
    "from torch_geometric.data import DataListLoader\n",
    "from random import shuffle\n",
    "from networkx.algorithms.approximation import max_clique\n",
    "from networkx.algorithms import find_cliques\n",
    "from torch_geometric.nn.norm import graph_size_norm\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch_geometric.nn import GatedGraphConv, GINEConv\n",
    "import os\n",
    "from pysat.solvers import Lingeling,Minisat22, Glucose4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_dense_to_sparse(tensor):\n",
    "    r\"\"\"Converts a dense adjacency matrix to a sparse adjacency matrix defined\n",
    "    by edge indices and edge attributes.\n",
    "\n",
    "    Args:\n",
    "        tensor (Tensor): The dense adjacency matrix.\n",
    "     :rtype: (:class:`LongTensor`, :class:`Tensor`)\n",
    "    \"\"\"\n",
    "    assert tensor.dim() == 2\n",
    "    index = tensor.nonzero(as_tuple=False).t().contiguous()\n",
    "    value = tensor[index[0], index[1]]\n",
    "    return index, value            \n",
    "                        \n",
    "def generate_random_kCNF(k,num_variables, num_clauses):\n",
    "    formula = CNF()\n",
    "    for clause_i in range(num_clauses):\n",
    "        clause = []\n",
    "        for literal in range(k):\n",
    "            #randint is not inclusive on the upper bound, hence +1\n",
    "            literal = np.random.randint(-num_variables,num_variables+1)\n",
    "            while (literal in clause) or (-literal in clause) or (literal==0):\n",
    "                    literal = np.random.randint(-num_variables,num_variables+1)\n",
    "            clause += [literal]\n",
    "        formula.append(clause)\n",
    "    return formula\n",
    "\n",
    "#detect dependencies between clauses (2 clauses have a dependency if they share a variable)\n",
    "#dependencies are lopsided, a variable and its negative will connect two clauses with a negative sign\n",
    "def detect_dependency_2(clause_1, clause_2):\n",
    "    dependency_score = 0\n",
    "    for  var_1 in clause_1:\n",
    "        for var_2 in clause_2:\n",
    "            if np.abs(var_1) == np.abs(var_2):\n",
    "                if np.sign(var_1)==np.sign(var_2):\n",
    "                    return 1\n",
    "                else:\n",
    "                    return -1\n",
    "            \n",
    "    return 0\n",
    "\n",
    "\n",
    "def detect_dependency(clause_1, clause_2):\n",
    "    dependency_score = 0\n",
    "    for  var_1 in clause_1:\n",
    "        for var_2 in clause_2:\n",
    "            if np.abs(var_1) == np.abs(var_2):\n",
    "                if np.sign(var_1)==np.sign(var_2):\n",
    "                    dependency_score = 1\n",
    "                else:\n",
    "                    return -1\n",
    "            \n",
    "    return dependency_score\n",
    "\n",
    "def get_VarClause_matrix(formula,k):\n",
    "    clause_var_adj = np.zeros((len(formula.clauses)+formula.nv,len(formula.clauses)+formula.nv))\n",
    "    clause_var_dir = np.zeros((formula.nv,formula.nv+len(formula.clauses)))\n",
    "    for i in range(len(formula.clauses)):\n",
    "        for j in range(k):\n",
    "            literal = formula.clauses[i][j] \n",
    "            clause_var_adj[i,np.abs(literal)-1] =  np.sign(literal)\n",
    "            clause_var_dir[np.abs(literal)-1,i+formula.nv] = np.sign(literal)\n",
    "    return clause_var_adj, clause_var_dir\n",
    "    \n",
    "def get_VarClause_matrix_2(formula,k):\n",
    "    clause_var_adj = np.zeros((len(formula.clauses)+formula.nv,len(formula.clauses)+formula.nv))\n",
    "    clause_var_dir = np.zeros((formula.nv,formula.nv+len(formula.clauses)))\n",
    "    for i in range(len(formula.clauses)):\n",
    "        for j in range(k):\n",
    "            literal = formula.clauses[i][j] \n",
    "            clause_var_adj[i,np.abs(literal)-1] =  np.sign(literal)\n",
    "            clause_var_dir[np.abs(literal)-1,i+formula.nv] = np.sign(literal)\n",
    "    return clause_var_adj, clause_var_dir\n",
    "    \n",
    "    \n",
    "  \n",
    "def get_dependency_matrix(formula):\n",
    "    num_clauses = len(formula.clauses)\n",
    "    dependency_graph_adj = np.zeros((len(formula.clauses)+formula.nv,len(formula.clauses)+formula.nv))\n",
    "    for row in range(num_clauses):\n",
    "        for column in range(num_clauses):\n",
    "            dependency_graph_adj[row+formula.nv,column+formula.nv] = detect_dependency(formula.clauses[row], formula.clauses[column])\n",
    "    return dependency_graph_adj- np.eye(dependency_graph_adj.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "def generate_dimacs_CNFDataset(dimacs_dataset, k):\n",
    "    dataset=[]\n",
    "    \n",
    "    for (counter,formula) in enumerate(dimacs_dataset):\n",
    "\n",
    "        with Glucose4(bootstrap_with=formula.clauses, with_proof=True) as l:\n",
    "            is_sat = (l.solve())\n",
    "            \n",
    "        num_variables = len(set([np.abs(item) for sublist in (formula.clauses) for item in sublist]))\n",
    "        num_clauses  = len(formula.clauses)\n",
    "\n",
    "        #adj_matrix = get_dependency_matrix(formula)\n",
    "        var_clause_mat, var_clause_dirmat = get_VarClause_matrix(formula,k)\n",
    "        \n",
    "        #delete isolated node rows,columns from clause-clause graph and var-clause graph\n",
    "        #adj_matrix = np.delete(adj_matrix,np.where((np.abs(var_clause_dirmat).sum(1)[:formula.nv]==0))[0],axis=0)\n",
    "        #adj_matrix = np.delete(adj_matrix,np.where((np.abs(var_clause_dirmat).sum(1)[:formula.nv]==0))[0],axis=1)\n",
    "\n",
    "\n",
    "        var_clause_dirmat_temp = np.delete(var_clause_dirmat,np.where((np.abs(var_clause_dirmat).sum(1)[:formula.nv]==0))[0],axis=0)\n",
    "        var_clause_dirmat_temp = np.delete(var_clause_dirmat_temp,np.where((np.abs(var_clause_dirmat).sum(1)[:formula.nv]==0))[0],axis=1)\n",
    "\n",
    "        var_clause_dirmat = var_clause_dirmat_temp\n",
    "        \n",
    "                #print(var_clause_dirmat.shape)\n",
    "        vc_edge_ind, vc_edge_attr = old_dense_to_sparse(torch.tensor(var_clause_dirmat))\n",
    "        #dirvc_edge_ind, dirvc_edge_attr = dense_to_sparse(torch.tensor(var_clause_dirmat))\n",
    "        #print(\"it's undirected: \", is_undirected(vc_edge_ind))\n",
    "        x_var = torch.zeros(num_variables)\n",
    "        x_clause = torch.ones(num_clauses)\n",
    "        x_comb = torch.cat([x_var,x_clause])\n",
    "        #cc_edge_ind, cc_edge_attr = old_dense_to_sparse(torch.tensor(adj_matrix))\n",
    "\n",
    "\n",
    "        graph_object = Data(x= x_comb, edge_index = vc_edge_ind, edge_attr = vc_edge_attr, formula = [formula],  sat=is_sat)\n",
    "\n",
    "#        graph_object = Data(x= x_comb, edge_index = vc_edge_ind, edge_attr = vc_edge_attr, cc_edge_index = cc_edge_ind, cc_edge_att =cc_edge_attr,  formula = [formula],  sat=is_sat)\n",
    "        dataset += [graph_object]\n",
    "        print(\"current graph: \", counter)\n",
    "    return dataset\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_RandomCNFDataset(num_graphs, k, num_variables_low, num_clauses_low, num_variables_high=None, num_clauses_high=None, verbose = True):\n",
    "    dataset=[]\n",
    "    for i in range(num_graphs):\n",
    "        if num_variables_high is None:\n",
    "            num_variables = num_variables_low\n",
    "        else:\n",
    "            num_variables = np.random.randint(num_variables_low, num_variables_high)\n",
    "            \n",
    "        if num_clauses_high is None:\n",
    "            num_clauses = num_clauses_low \n",
    "        else:\n",
    "            num_clauses = np.random.randint(num_clauses_low, num_clauses_high)\n",
    "\n",
    "        num_variables = np.random.randint(num_variables_low, num_variables_high)        \n",
    "        formula = generate_random_kCNF(k, num_variables,num_clauses)\n",
    "\n",
    "        \n",
    "        with Lingeling(bootstrap_with=formula.clauses, with_proof=True) as l:\n",
    "            is_sat = (l.solve())\n",
    "            \n",
    "        num_variables = len(set([np.abs(item) for sublist in (formula.clauses) for item in sublist]))\n",
    "        num_clauses  = len(formula.clauses)\n",
    "\n",
    "        #adj_matrix = get_dependency_matrix(formula)\n",
    "        var_clause_mat, var_clause_dirmat = get_VarClause_matrix(formula,k)\n",
    "    \n",
    "        \n",
    "        # #delete isolated node rows,columns from clause-clause graph and var-clause graph\n",
    "        # adj_matrix = np.delete(adj_matrix,np.where((np.abs(var_clause_dirmat).sum(1)[:formula.nv]==0))[0],axis=0)\n",
    "        # adj_matrix = np.delete(adj_matrix,np.where((np.abs(var_clause_dirmat).sum(1)[:formula.nv]==0))[0],axis=1)\n",
    "\n",
    "\n",
    "        var_clause_dirmat_temp = np.delete(var_clause_dirmat,np.where((np.abs(var_clause_dirmat).sum(1)[:formula.nv]==0))[0],axis=0)\n",
    "        var_clause_dirmat_temp = np.delete(var_clause_dirmat_temp,np.where((np.abs(var_clause_dirmat).sum(1)[:formula.nv]==0))[0],axis=1)\n",
    "\n",
    "        var_clause_dirmat = var_clause_dirmat_temp\n",
    "\n",
    "\n",
    "        #print(var_clause_dirmat.shape)\n",
    "        vc_edge_ind, vc_edge_attr = old_dense_to_sparse(torch.tensor(var_clause_dirmat))\n",
    "        #dirvc_edge_ind, dirvc_edge_attr = dense_to_sparse(torch.tensor(var_clause_dirmat))\n",
    "        #print(\"it's undirected: \", is_undirected(vc_edge_ind))\n",
    "        x_var = torch.zeros(num_variables)\n",
    "        x_clause = torch.ones(num_clauses)\n",
    "        x_comb = torch.cat([x_var,x_clause])\n",
    "        #cc_edge_ind, cc_edge_attr = old_dense_to_sparse(torch.tensor(adj_matrix))\n",
    "        \n",
    "#         if contains_isolated_nodes(vc_edge_ind):\n",
    "#             print(np.where((np.abs(var_clause_dirmat).sum(1)[:formula.nv]==0))[0])\n",
    "#             print(\"contains isolated nodes\")\n",
    "#             breakpoint()\n",
    "        graph_object = Data(x= x_comb, edge_index = vc_edge_ind, edge_attr = vc_edge_attr, formula = [formula],  sat=is_sat)\n",
    "#       graph_object = Data(x= x_comb, edge_index = vc_edge_ind, edge_attr = vc_edge_attr, cc_edge_index = cc_edge_ind, cc_edge_att =cc_edge_attr,  formula = [formula],  sat=is_sat)\n",
    "\n",
    "        dataset += [graph_object]\n",
    "        if verbose:\n",
    "            print(\"current graph: \", i)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dimacs_directory(directory):\n",
    "    directory_contents = os.listdir(directory)\n",
    "    dataset_formulas = []\n",
    "    #print(directory_contents)\n",
    "    for k in range(len(directory_contents)):\n",
    "        if \"txt\" in directory_contents[k].split()[0]:\n",
    "            formula = CNF(from_file= directory + '/' +directory_contents[k])\n",
    "            #print(formula)\n",
    "            dataset_formulas += [formula]  \n",
    "    return dataset_formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERATE OR LOAD DATASET HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current graph:  0\n",
      "current graph:  1\n",
      "current graph:  2\n",
      "current graph:  3\n",
      "current graph:  4\n",
      "current graph:  5\n",
      "current graph:  6\n",
      "current graph:  7\n",
      "current graph:  8\n",
      "current graph:  9\n",
      "current graph:  10\n",
      "current graph:  11\n",
      "current graph:  12\n",
      "current graph:  13\n",
      "current graph:  14\n",
      "current graph:  15\n",
      "current graph:  16\n",
      "current graph:  17\n",
      "current graph:  18\n",
      "current graph:  19\n",
      "current graph:  20\n",
      "current graph:  21\n",
      "current graph:  22\n",
      "current graph:  23\n",
      "current graph:  24\n",
      "current graph:  25\n",
      "current graph:  26\n",
      "current graph:  27\n",
      "current graph:  28\n",
      "current graph:  29\n",
      "current graph:  30\n",
      "current graph:  31\n",
      "current graph:  32\n",
      "current graph:  33\n",
      "current graph:  34\n",
      "current graph:  35\n",
      "current graph:  36\n",
      "current graph:  37\n",
      "current graph:  38\n",
      "current graph:  39\n",
      "current graph:  40\n",
      "current graph:  41\n",
      "current graph:  42\n",
      "current graph:  43\n",
      "current graph:  44\n",
      "current graph:  45\n",
      "current graph:  46\n",
      "current graph:  47\n",
      "current graph:  48\n",
      "current graph:  49\n",
      "current graph:  50\n",
      "current graph:  51\n",
      "current graph:  52\n",
      "current graph:  53\n",
      "current graph:  54\n",
      "current graph:  55\n",
      "current graph:  56\n",
      "current graph:  57\n",
      "current graph:  58\n",
      "current graph:  59\n",
      "current graph:  60\n",
      "current graph:  61\n",
      "current graph:  62\n",
      "current graph:  63\n",
      "current graph:  64\n",
      "current graph:  65\n",
      "current graph:  66\n",
      "current graph:  67\n",
      "current graph:  68\n",
      "current graph:  69\n",
      "current graph:  70\n",
      "current graph:  71\n",
      "current graph:  72\n",
      "current graph:  73\n",
      "current graph:  74\n",
      "current graph:  75\n",
      "current graph:  76\n",
      "current graph:  77\n",
      "current graph:  78\n",
      "current graph:  79\n",
      "current graph:  80\n",
      "current graph:  81\n",
      "current graph:  82\n",
      "current graph:  83\n",
      "current graph:  84\n",
      "current graph:  85\n",
      "current graph:  86\n",
      "current graph:  87\n",
      "current graph:  88\n",
      "current graph:  89\n",
      "current graph:  90\n",
      "current graph:  91\n",
      "current graph:  92\n",
      "current graph:  93\n",
      "current graph:  94\n",
      "current graph:  95\n",
      "current graph:  96\n",
      "current graph:  97\n",
      "current graph:  98\n",
      "current graph:  99\n",
      "current graph:  100\n",
      "current graph:  101\n",
      "current graph:  102\n",
      "current graph:  103\n",
      "current graph:  104\n",
      "current graph:  105\n",
      "current graph:  106\n",
      "current graph:  107\n",
      "current graph:  108\n",
      "current graph:  109\n",
      "current graph:  110\n",
      "current graph:  111\n",
      "current graph:  112\n",
      "current graph:  113\n",
      "current graph:  114\n",
      "current graph:  115\n",
      "current graph:  116\n",
      "current graph:  117\n",
      "current graph:  118\n",
      "current graph:  119\n",
      "current graph:  120\n",
      "current graph:  121\n",
      "current graph:  122\n",
      "current graph:  123\n",
      "current graph:  124\n",
      "current graph:  125\n",
      "current graph:  126\n",
      "current graph:  127\n",
      "current graph:  128\n",
      "current graph:  129\n",
      "current graph:  130\n",
      "current graph:  131\n",
      "current graph:  132\n",
      "current graph:  133\n",
      "current graph:  134\n",
      "current graph:  135\n",
      "current graph:  136\n",
      "current graph:  137\n",
      "current graph:  138\n",
      "current graph:  139\n",
      "current graph:  140\n",
      "current graph:  141\n",
      "current graph:  142\n",
      "current graph:  143\n",
      "current graph:  144\n",
      "current graph:  145\n",
      "current graph:  146\n",
      "current graph:  147\n",
      "current graph:  148\n",
      "current graph:  149\n",
      "current graph:  150\n",
      "current graph:  151\n",
      "current graph:  152\n",
      "current graph:  153\n",
      "current graph:  154\n",
      "current graph:  155\n",
      "current graph:  156\n",
      "current graph:  157\n",
      "current graph:  158\n",
      "current graph:  159\n",
      "current graph:  160\n",
      "current graph:  161\n",
      "current graph:  162\n",
      "current graph:  163\n",
      "current graph:  164\n",
      "current graph:  165\n",
      "current graph:  166\n",
      "current graph:  167\n",
      "current graph:  168\n",
      "current graph:  169\n",
      "current graph:  170\n",
      "current graph:  171\n",
      "current graph:  172\n",
      "current graph:  173\n",
      "current graph:  174\n",
      "current graph:  175\n",
      "current graph:  176\n",
      "current graph:  177\n",
      "current graph:  178\n",
      "current graph:  179\n",
      "current graph:  180\n",
      "current graph:  181\n",
      "current graph:  182\n",
      "current graph:  183\n",
      "current graph:  184\n",
      "current graph:  185\n",
      "current graph:  186\n",
      "current graph:  187\n",
      "current graph:  188\n",
      "current graph:  189\n",
      "current graph:  190\n",
      "current graph:  191\n",
      "current graph:  192\n",
      "current graph:  193\n",
      "current graph:  194\n",
      "current graph:  195\n",
      "current graph:  196\n",
      "current graph:  197\n",
      "current graph:  198\n",
      "current graph:  199\n",
      "current graph:  200\n",
      "current graph:  201\n",
      "current graph:  202\n",
      "current graph:  203\n",
      "current graph:  204\n",
      "current graph:  205\n",
      "current graph:  206\n",
      "current graph:  207\n",
      "current graph:  208\n",
      "current graph:  209\n",
      "current graph:  210\n",
      "current graph:  211\n",
      "current graph:  212\n",
      "current graph:  213\n",
      "current graph:  214\n",
      "current graph:  215\n",
      "current graph:  216\n",
      "current graph:  217\n",
      "current graph:  218\n",
      "current graph:  219\n",
      "current graph:  220\n",
      "current graph:  221\n",
      "current graph:  222\n",
      "current graph:  223\n",
      "current graph:  224\n",
      "current graph:  225\n",
      "current graph:  226\n",
      "current graph:  227\n",
      "current graph:  228\n",
      "current graph:  229\n",
      "current graph:  230\n",
      "current graph:  231\n",
      "current graph:  232\n",
      "current graph:  233\n",
      "current graph:  234\n",
      "current graph:  235\n",
      "current graph:  236\n",
      "current graph:  237\n",
      "current graph:  238\n",
      "current graph:  239\n",
      "current graph:  240\n",
      "current graph:  241\n",
      "current graph:  242\n",
      "current graph:  243\n",
      "current graph:  244\n",
      "current graph:  245\n",
      "current graph:  246\n",
      "current graph:  247\n",
      "current graph:  248\n",
      "current graph:  249\n",
      "current graph:  250\n",
      "current graph:  251\n",
      "current graph:  252\n",
      "current graph:  253\n",
      "current graph:  254\n",
      "current graph:  255\n",
      "current graph:  256\n",
      "current graph:  257\n",
      "current graph:  258\n",
      "current graph:  259\n",
      "current graph:  260\n",
      "current graph:  261\n",
      "current graph:  262\n",
      "current graph:  263\n",
      "current graph:  264\n",
      "current graph:  265\n",
      "current graph:  266\n",
      "current graph:  267\n",
      "current graph:  268\n",
      "current graph:  269\n",
      "current graph:  270\n",
      "current graph:  271\n",
      "current graph:  272\n",
      "current graph:  273\n",
      "current graph:  274\n",
      "current graph:  275\n",
      "current graph:  276\n",
      "current graph:  277\n",
      "current graph:  278\n",
      "current graph:  279\n",
      "current graph:  280\n",
      "current graph:  281\n",
      "current graph:  282\n",
      "current graph:  283\n",
      "current graph:  284\n",
      "current graph:  285\n",
      "current graph:  286\n",
      "current graph:  287\n",
      "current graph:  288\n",
      "current graph:  289\n",
      "current graph:  290\n",
      "current graph:  291\n",
      "current graph:  292\n",
      "current graph:  293\n",
      "current graph:  294\n",
      "current graph:  295\n",
      "current graph:  296\n",
      "current graph:  297\n",
      "current graph:  298\n",
      "current graph:  299\n",
      "current graph:  300\n",
      "current graph:  301\n",
      "current graph:  302\n",
      "current graph:  303\n",
      "current graph:  304\n",
      "current graph:  305\n",
      "current graph:  306\n",
      "current graph:  307\n",
      "current graph:  308\n",
      "current graph:  309\n",
      "current graph:  310\n",
      "current graph:  311\n",
      "current graph:  312\n",
      "current graph:  313\n",
      "current graph:  314\n",
      "current graph:  315\n",
      "current graph:  316\n",
      "current graph:  317\n",
      "current graph:  318\n",
      "current graph:  319\n",
      "current graph:  320\n",
      "current graph:  321\n",
      "current graph:  322\n",
      "current graph:  323\n",
      "current graph:  324\n",
      "current graph:  325\n",
      "current graph:  326\n",
      "current graph:  327\n",
      "current graph:  328\n",
      "current graph:  329\n",
      "current graph:  330\n",
      "current graph:  331\n",
      "current graph:  332\n",
      "current graph:  333\n",
      "current graph:  334\n",
      "current graph:  335\n",
      "current graph:  336\n",
      "current graph:  337\n",
      "current graph:  338\n",
      "current graph:  339\n",
      "current graph:  340\n",
      "current graph:  341\n",
      "current graph:  342\n",
      "current graph:  343\n",
      "current graph:  344\n",
      "current graph:  345\n",
      "current graph:  346\n",
      "current graph:  347\n",
      "current graph:  348\n",
      "current graph:  349\n",
      "current graph:  350\n",
      "current graph:  351\n",
      "current graph:  352\n",
      "current graph:  353\n",
      "current graph:  354\n",
      "current graph:  355\n",
      "current graph:  356\n",
      "current graph:  357\n",
      "current graph:  358\n",
      "current graph:  359\n",
      "current graph:  360\n",
      "current graph:  361\n",
      "current graph:  362\n",
      "current graph:  363\n",
      "current graph:  364\n",
      "current graph:  365\n",
      "current graph:  366\n",
      "current graph:  367\n",
      "current graph:  368\n",
      "current graph:  369\n",
      "current graph:  370\n",
      "current graph:  371\n",
      "current graph:  372\n",
      "current graph:  373\n",
      "current graph:  374\n",
      "current graph:  375\n",
      "current graph:  376\n",
      "current graph:  377\n",
      "current graph:  378\n",
      "current graph:  379\n",
      "current graph:  380\n",
      "current graph:  381\n",
      "current graph:  382\n",
      "current graph:  383\n",
      "current graph:  384\n",
      "current graph:  385\n",
      "current graph:  386\n",
      "current graph:  387\n",
      "current graph:  388\n",
      "current graph:  389\n",
      "current graph:  390\n",
      "current graph:  391\n",
      "current graph:  392\n",
      "current graph:  393\n",
      "current graph:  394\n",
      "current graph:  395\n",
      "current graph:  396\n",
      "current graph:  397\n",
      "current graph:  398\n",
      "current graph:  399\n",
      "current graph:  400\n",
      "current graph:  401\n",
      "current graph:  402\n",
      "current graph:  403\n",
      "current graph:  404\n",
      "current graph:  405\n",
      "current graph:  406\n",
      "current graph:  407\n",
      "current graph:  408\n",
      "current graph:  409\n",
      "current graph:  410\n",
      "current graph:  411\n",
      "current graph:  412\n",
      "current graph:  413\n",
      "current graph:  414\n",
      "current graph:  415\n",
      "current graph:  416\n",
      "current graph:  417\n",
      "current graph:  418\n",
      "current graph:  419\n",
      "current graph:  420\n",
      "current graph:  421\n",
      "current graph:  422\n",
      "current graph:  423\n",
      "current graph:  424\n",
      "current graph:  425\n",
      "current graph:  426\n",
      "current graph:  427\n",
      "current graph:  428\n",
      "current graph:  429\n",
      "current graph:  430\n",
      "current graph:  431\n",
      "current graph:  432\n",
      "current graph:  433\n",
      "current graph:  434\n",
      "current graph:  435\n",
      "current graph:  436\n",
      "current graph:  437\n",
      "current graph:  438\n",
      "current graph:  439\n",
      "current graph:  440\n",
      "current graph:  441\n",
      "current graph:  442\n",
      "current graph:  443\n",
      "current graph:  444\n",
      "current graph:  445\n",
      "current graph:  446\n",
      "current graph:  447\n",
      "current graph:  448\n",
      "current graph:  449\n",
      "current graph:  450\n",
      "current graph:  451\n",
      "current graph:  452\n",
      "current graph:  453\n",
      "current graph:  454\n",
      "current graph:  455\n",
      "current graph:  456\n",
      "current graph:  457\n",
      "current graph:  458\n",
      "current graph:  459\n",
      "current graph:  460\n",
      "current graph:  461\n",
      "current graph:  462\n",
      "current graph:  463\n",
      "current graph:  464\n",
      "current graph:  465\n",
      "current graph:  466\n",
      "current graph:  467\n",
      "current graph:  468\n",
      "current graph:  469\n",
      "current graph:  470\n",
      "current graph:  471\n",
      "current graph:  472\n",
      "current graph:  473\n",
      "current graph:  474\n",
      "current graph:  475\n",
      "current graph:  476\n",
      "current graph:  477\n",
      "current graph:  478\n",
      "current graph:  479\n",
      "current graph:  480\n",
      "current graph:  481\n",
      "current graph:  482\n",
      "current graph:  483\n",
      "current graph:  484\n",
      "current graph:  485\n",
      "current graph:  486\n",
      "current graph:  487\n",
      "current graph:  488\n",
      "current graph:  489\n",
      "current graph:  490\n",
      "current graph:  491\n",
      "current graph:  492\n",
      "current graph:  493\n",
      "current graph:  494\n",
      "current graph:  495\n",
      "current graph:  496\n",
      "current graph:  497\n",
      "current graph:  498\n",
      "current graph:  499\n",
      "current graph:  500\n",
      "current graph:  501\n",
      "current graph:  502\n",
      "current graph:  503\n",
      "current graph:  504\n",
      "current graph:  505\n",
      "current graph:  506\n",
      "current graph:  507\n",
      "current graph:  508\n",
      "current graph:  509\n",
      "current graph:  510\n",
      "current graph:  511\n",
      "current graph:  512\n",
      "current graph:  513\n",
      "current graph:  514\n",
      "current graph:  515\n",
      "current graph:  516\n",
      "current graph:  517\n",
      "current graph:  518\n",
      "current graph:  519\n",
      "current graph:  520\n",
      "current graph:  521\n",
      "current graph:  522\n",
      "current graph:  523\n",
      "current graph:  524\n",
      "current graph:  525\n",
      "current graph:  526\n",
      "current graph:  527\n",
      "current graph:  528\n",
      "current graph:  529\n",
      "current graph:  530\n",
      "current graph:  531\n",
      "current graph:  532\n",
      "current graph:  533\n",
      "current graph:  534\n",
      "current graph:  535\n",
      "current graph:  536\n",
      "current graph:  537\n",
      "current graph:  538\n",
      "current graph:  539\n",
      "current graph:  540\n",
      "current graph:  541\n",
      "current graph:  542\n",
      "current graph:  543\n",
      "current graph:  544\n",
      "current graph:  545\n",
      "current graph:  546\n",
      "current graph:  547\n",
      "current graph:  548\n",
      "current graph:  549\n",
      "current graph:  550\n",
      "current graph:  551\n",
      "current graph:  552\n",
      "current graph:  553\n",
      "current graph:  554\n",
      "current graph:  555\n",
      "current graph:  556\n",
      "current graph:  557\n",
      "current graph:  558\n",
      "current graph:  559\n",
      "current graph:  560\n",
      "current graph:  561\n",
      "current graph:  562\n",
      "current graph:  563\n",
      "current graph:  564\n",
      "current graph:  565\n",
      "current graph:  566\n",
      "current graph:  567\n",
      "current graph:  568\n",
      "current graph:  569\n",
      "current graph:  570\n",
      "current graph:  571\n",
      "current graph:  572\n",
      "current graph:  573\n",
      "current graph:  574\n",
      "current graph:  575\n",
      "current graph:  576\n",
      "current graph:  577\n",
      "current graph:  578\n",
      "current graph:  579\n",
      "current graph:  580\n",
      "current graph:  581\n",
      "current graph:  582\n",
      "current graph:  583\n",
      "current graph:  584\n",
      "current graph:  585\n",
      "current graph:  586\n",
      "current graph:  587\n",
      "current graph:  588\n",
      "current graph:  589\n",
      "current graph:  590\n",
      "current graph:  591\n",
      "current graph:  592\n",
      "current graph:  593\n",
      "current graph:  594\n",
      "current graph:  595\n",
      "current graph:  596\n",
      "current graph:  597\n",
      "current graph:  598\n",
      "current graph:  599\n",
      "current graph:  600\n",
      "current graph:  601\n",
      "current graph:  602\n",
      "current graph:  603\n",
      "current graph:  604\n",
      "current graph:  605\n",
      "current graph:  606\n",
      "current graph:  607\n",
      "current graph:  608\n",
      "current graph:  609\n",
      "current graph:  610\n",
      "current graph:  611\n",
      "current graph:  612\n",
      "current graph:  613\n",
      "current graph:  614\n",
      "current graph:  615\n",
      "current graph:  616\n",
      "current graph:  617\n",
      "current graph:  618\n",
      "current graph:  619\n",
      "current graph:  620\n",
      "current graph:  621\n",
      "current graph:  622\n",
      "current graph:  623\n",
      "current graph:  624\n",
      "current graph:  625\n",
      "current graph:  626\n",
      "current graph:  627\n",
      "current graph:  628\n",
      "current graph:  629\n",
      "current graph:  630\n",
      "current graph:  631\n",
      "current graph:  632\n",
      "current graph:  633\n",
      "current graph:  634\n",
      "current graph:  635\n",
      "current graph:  636\n",
      "current graph:  637\n",
      "current graph:  638\n",
      "current graph:  639\n",
      "current graph:  640\n",
      "current graph:  641\n",
      "current graph:  642\n",
      "current graph:  643\n",
      "current graph:  644\n",
      "current graph:  645\n",
      "current graph:  646\n",
      "current graph:  647\n",
      "current graph:  648\n",
      "current graph:  649\n",
      "current graph:  650\n",
      "current graph:  651\n",
      "current graph:  652\n",
      "current graph:  653\n",
      "current graph:  654\n",
      "current graph:  655\n",
      "current graph:  656\n",
      "current graph:  657\n",
      "current graph:  658\n",
      "current graph:  659\n",
      "current graph:  660\n",
      "current graph:  661\n",
      "current graph:  662\n",
      "current graph:  663\n",
      "current graph:  664\n",
      "current graph:  665\n",
      "current graph:  666\n",
      "current graph:  667\n",
      "current graph:  668\n",
      "current graph:  669\n",
      "current graph:  670\n",
      "current graph:  671\n",
      "current graph:  672\n",
      "current graph:  673\n",
      "current graph:  674\n",
      "current graph:  675\n",
      "current graph:  676\n",
      "current graph:  677\n",
      "current graph:  678\n",
      "current graph:  679\n",
      "current graph:  680\n",
      "current graph:  681\n",
      "current graph:  682\n",
      "current graph:  683\n",
      "current graph:  684\n",
      "current graph:  685\n",
      "current graph:  686\n",
      "current graph:  687\n",
      "current graph:  688\n",
      "current graph:  689\n",
      "current graph:  690\n",
      "current graph:  691\n",
      "current graph:  692\n",
      "current graph:  693\n",
      "current graph:  694\n",
      "current graph:  695\n",
      "current graph:  696\n",
      "current graph:  697\n",
      "current graph:  698\n",
      "current graph:  699\n",
      "current graph:  700\n",
      "current graph:  701\n",
      "current graph:  702\n",
      "current graph:  703\n",
      "current graph:  704\n",
      "current graph:  705\n",
      "current graph:  706\n",
      "current graph:  707\n",
      "current graph:  708\n",
      "current graph:  709\n",
      "current graph:  710\n",
      "current graph:  711\n",
      "current graph:  712\n",
      "current graph:  713\n",
      "current graph:  714\n",
      "current graph:  715\n",
      "current graph:  716\n",
      "current graph:  717\n",
      "current graph:  718\n",
      "current graph:  719\n",
      "current graph:  720\n",
      "current graph:  721\n",
      "current graph:  722\n",
      "current graph:  723\n",
      "current graph:  724\n",
      "current graph:  725\n",
      "current graph:  726\n",
      "current graph:  727\n",
      "current graph:  728\n",
      "current graph:  729\n",
      "current graph:  730\n",
      "current graph:  731\n",
      "current graph:  732\n",
      "current graph:  733\n",
      "current graph:  734\n",
      "current graph:  735\n",
      "current graph:  736\n",
      "current graph:  737\n",
      "current graph:  738\n",
      "current graph:  739\n",
      "current graph:  740\n",
      "current graph:  741\n",
      "current graph:  742\n",
      "current graph:  743\n",
      "current graph:  744\n",
      "current graph:  745\n",
      "current graph:  746\n",
      "current graph:  747\n",
      "current graph:  748\n",
      "current graph:  749\n",
      "current graph:  750\n",
      "current graph:  751\n",
      "current graph:  752\n",
      "current graph:  753\n",
      "current graph:  754\n",
      "current graph:  755\n",
      "current graph:  756\n",
      "current graph:  757\n",
      "current graph:  758\n",
      "current graph:  759\n",
      "current graph:  760\n",
      "current graph:  761\n",
      "current graph:  762\n",
      "current graph:  763\n",
      "current graph:  764\n",
      "current graph:  765\n",
      "current graph:  766\n",
      "current graph:  767\n",
      "current graph:  768\n",
      "current graph:  769\n",
      "current graph:  770\n",
      "current graph:  771\n",
      "current graph:  772\n",
      "current graph:  773\n",
      "current graph:  774\n",
      "current graph:  775\n",
      "current graph:  776\n",
      "current graph:  777\n",
      "current graph:  778\n",
      "current graph:  779\n",
      "current graph:  780\n",
      "current graph:  781\n",
      "current graph:  782\n",
      "current graph:  783\n",
      "current graph:  784\n",
      "current graph:  785\n",
      "current graph:  786\n",
      "current graph:  787\n",
      "current graph:  788\n",
      "current graph:  789\n",
      "current graph:  790\n",
      "current graph:  791\n",
      "current graph:  792\n",
      "current graph:  793\n",
      "current graph:  794\n",
      "current graph:  795\n",
      "current graph:  796\n",
      "current graph:  797\n",
      "current graph:  798\n",
      "current graph:  799\n",
      "current graph:  800\n",
      "current graph:  801\n",
      "current graph:  802\n",
      "current graph:  803\n",
      "current graph:  804\n",
      "current graph:  805\n",
      "current graph:  806\n",
      "current graph:  807\n",
      "current graph:  808\n",
      "current graph:  809\n",
      "current graph:  810\n",
      "current graph:  811\n",
      "current graph:  812\n",
      "current graph:  813\n",
      "current graph:  814\n",
      "current graph:  815\n",
      "current graph:  816\n",
      "current graph:  817\n",
      "current graph:  818\n",
      "current graph:  819\n",
      "current graph:  820\n",
      "current graph:  821\n",
      "current graph:  822\n",
      "current graph:  823\n",
      "current graph:  824\n",
      "current graph:  825\n",
      "current graph:  826\n",
      "current graph:  827\n",
      "current graph:  828\n",
      "current graph:  829\n",
      "current graph:  830\n",
      "current graph:  831\n",
      "current graph:  832\n",
      "current graph:  833\n",
      "current graph:  834\n",
      "current graph:  835\n",
      "current graph:  836\n",
      "current graph:  837\n",
      "current graph:  838\n",
      "current graph:  839\n",
      "current graph:  840\n",
      "current graph:  841\n",
      "current graph:  842\n",
      "current graph:  843\n",
      "current graph:  844\n",
      "current graph:  845\n",
      "current graph:  846\n",
      "current graph:  847\n",
      "current graph:  848\n",
      "current graph:  849\n",
      "current graph:  850\n",
      "current graph:  851\n",
      "current graph:  852\n",
      "current graph:  853\n",
      "current graph:  854\n",
      "current graph:  855\n",
      "current graph:  856\n",
      "current graph:  857\n",
      "current graph:  858\n",
      "current graph:  859\n",
      "current graph:  860\n",
      "current graph:  861\n",
      "current graph:  862\n",
      "current graph:  863\n",
      "current graph:  864\n",
      "current graph:  865\n",
      "current graph:  866\n",
      "current graph:  867\n",
      "current graph:  868\n",
      "current graph:  869\n",
      "current graph:  870\n",
      "current graph:  871\n",
      "current graph:  872\n",
      "current graph:  873\n",
      "current graph:  874\n",
      "current graph:  875\n",
      "current graph:  876\n",
      "current graph:  877\n",
      "current graph:  878\n",
      "current graph:  879\n",
      "current graph:  880\n",
      "current graph:  881\n",
      "current graph:  882\n",
      "current graph:  883\n",
      "current graph:  884\n",
      "current graph:  885\n",
      "current graph:  886\n",
      "current graph:  887\n",
      "current graph:  888\n",
      "current graph:  889\n",
      "current graph:  890\n",
      "current graph:  891\n",
      "current graph:  892\n",
      "current graph:  893\n",
      "current graph:  894\n",
      "current graph:  895\n",
      "current graph:  896\n",
      "current graph:  897\n",
      "current graph:  898\n",
      "current graph:  899\n",
      "current graph:  900\n",
      "current graph:  901\n",
      "current graph:  902\n",
      "current graph:  903\n",
      "current graph:  904\n",
      "current graph:  905\n",
      "current graph:  906\n",
      "current graph:  907\n",
      "current graph:  908\n",
      "current graph:  909\n",
      "current graph:  910\n",
      "current graph:  911\n",
      "current graph:  912\n",
      "current graph:  913\n",
      "current graph:  914\n",
      "current graph:  915\n",
      "current graph:  916\n",
      "current graph:  917\n",
      "current graph:  918\n",
      "current graph:  919\n",
      "current graph:  920\n",
      "current graph:  921\n",
      "current graph:  922\n",
      "current graph:  923\n",
      "current graph:  924\n",
      "current graph:  925\n",
      "current graph:  926\n",
      "current graph:  927\n",
      "current graph:  928\n",
      "current graph:  929\n",
      "current graph:  930\n",
      "current graph:  931\n",
      "current graph:  932\n",
      "current graph:  933\n",
      "current graph:  934\n",
      "current graph:  935\n",
      "current graph:  936\n",
      "current graph:  937\n",
      "current graph:  938\n",
      "current graph:  939\n",
      "current graph:  940\n",
      "current graph:  941\n",
      "current graph:  942\n",
      "current graph:  943\n",
      "current graph:  944\n",
      "current graph:  945\n",
      "current graph:  946\n",
      "current graph:  947\n",
      "current graph:  948\n",
      "current graph:  949\n",
      "current graph:  950\n",
      "current graph:  951\n",
      "current graph:  952\n",
      "current graph:  953\n",
      "current graph:  954\n",
      "current graph:  955\n",
      "current graph:  956\n",
      "current graph:  957\n",
      "current graph:  958\n",
      "current graph:  959\n",
      "current graph:  960\n",
      "current graph:  961\n",
      "current graph:  962\n",
      "current graph:  963\n",
      "current graph:  964\n",
      "current graph:  965\n",
      "current graph:  966\n",
      "current graph:  967\n",
      "current graph:  968\n",
      "current graph:  969\n",
      "current graph:  970\n",
      "current graph:  971\n",
      "current graph:  972\n",
      "current graph:  973\n",
      "current graph:  974\n",
      "current graph:  975\n",
      "current graph:  976\n",
      "current graph:  977\n",
      "current graph:  978\n",
      "current graph:  979\n",
      "current graph:  980\n",
      "current graph:  981\n",
      "current graph:  982\n",
      "current graph:  983\n",
      "current graph:  984\n",
      "current graph:  985\n",
      "current graph:  986\n",
      "current graph:  987\n",
      "current graph:  988\n",
      "current graph:  989\n",
      "current graph:  990\n",
      "current graph:  991\n",
      "current graph:  992\n",
      "current graph:  993\n",
      "current graph:  994\n",
      "current graph:  995\n",
      "current graph:  996\n",
      "current graph:  997\n",
      "current graph:  998\n",
      "current graph:  999\n"
     ]
    }
   ],
   "source": [
    "parent_path = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "my_data = read_dimacs_directory(parent_path+'/datasets/dimacs_400_random/')\n",
    "test_dataset_torch = generate_dimacs_CNFDataset(my_data,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decide on model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pytorch_memlab import profile, mem_reporter\n",
    "#@profile\n",
    "class Erdos_MPNN(torch.nn.Module):\n",
    "    def __init__(self, num_layers, hidden1, hidden2,  num_iterations = 1):\n",
    "        super(Erdos_MPNN, self).__init__()\n",
    "        self.hidden1 = hidden1\n",
    "        self.hidden2 = hidden2\n",
    "        self.momentum = 0.1\n",
    "        self.num_iterations = num_iterations\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.numlayers = num_layers\n",
    "        self.heads = 1\n",
    "        self.concat = True        \n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bn = BN(self.heads*self.hidden1, momentum=self.momentum)\n",
    "        for i in range(num_layers-1):\n",
    "            self.bns.append(BN(self.heads*self.hidden1, momentum=self.momentum))\n",
    "\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()        \n",
    "        self.bn1 = BN(self.heads*self.hidden1)\n",
    "       \n",
    "        self.conv  = GatedGraphConv(self.hidden1,self.numlayers)\n",
    "        \n",
    "\n",
    "        self.conv1 = GINEConv(Sequential(Linear(self.hidden2,  self.heads*self.hidden1),\n",
    "            ReLU(),\n",
    "            Linear( self.heads*self.hidden1,  self.heads*self.hidden1),\n",
    "            ReLU(),\n",
    "            BN(self.heads*self.hidden1, momentum=self.momentum),\n",
    "        ),train_eps=False)\n",
    "        \n",
    "        self.x_lin_2 = Linear(self.hidden1,1)\n",
    "        self.x_lin_1 = Linear(self.hidden1,self.hidden1)\n",
    "        \n",
    "        self.x_batchnorm = BN(self.hidden1)\n",
    "        if self.concat:\n",
    "            self.lin1 = Linear(self.heads*self.hidden1, self.hidden1)\n",
    "        else:\n",
    "            self.lin1 = Linear(self.hidden1, self.hidden1)\n",
    "        self.lin2 = Linear(self.hidden1, 1)\n",
    "        self.gnorm = graph_size_norm.GraphSizeNorm()\n",
    "\n",
    "                    \n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        #self.conv_alt.reset_parameters()\n",
    "        #self.conv_alt_rec.reset_parameters()\n",
    "        self.conv.reset_parameters()\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "            \n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "            \n",
    "        self.bn1.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "       # self.x_lin_1.reset_parameters()\n",
    "        #self.x_lin_2.reset_parameters()\n",
    "#\n",
    "        # self.x_conv_probs.reset_parameters()\n",
    "        # self.x_conv_degs.reset_parameters()\n",
    "\n",
    "        \n",
    "        #self.x_conv_2.reset_parameters()\n",
    "        self.x_batchnorm.reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        x_index = data.x\n",
    "        batch = data.batch\n",
    "        num_graphs = batch.max().item() + 1\n",
    "\n",
    "#         print(\"degs shape\", degs.shape)\n",
    "#         print(\"data.x shape \", data.x.shape)\n",
    "        \n",
    "        #directed edge index\n",
    "        #print(is_undirected(data.edge_index))\n",
    "        d_row, d_col = data.edge_index.detach()\n",
    "        edge_attr = data.edge_attr.detach()\n",
    "        \n",
    "        pos_assoc = edge_attr*((edge_attr>0)*1.)\n",
    "        neg_assoc = edge_attr*((edge_attr<0)*1.)\n",
    "        \n",
    "        #make undirected\n",
    "        edge_index = torch.cat([data.edge_index,data.edge_index[[1,0],:]],dim=1)\n",
    "        edge_attr = torch.cat([edge_attr.unsqueeze(-1), edge_attr.unsqueeze(-1)],dim=0)\n",
    "\n",
    "        \n",
    "        select_clauses = (data.x==1)\n",
    "        select_variables = (data.x==0)\n",
    "\n",
    "        #CREATE UNDIRECTED EDGE_INDEX\n",
    "        row, col = edge_index\n",
    "        \n",
    "        #DANGEROUS\n",
    "        degs = degree(row)\n",
    "        x = degs*1.\n",
    "        \n",
    "\n",
    "        #other stuff\n",
    "        total_num_edges = edge_index.shape[1]\n",
    "        N_size = x.shape[0]\n",
    "        \n",
    "        # #positive and negative degrees of each node\n",
    "        posdeg= scatter(pos_assoc,d_col, dim=0, dim_size=data.x.shape[0], reduce=\"sum\")+scatter(pos_assoc,d_row, dim=0, dim_size=data.x.shape[0], reduce=\"sum\")\n",
    "        negdeg= scatter(neg_assoc,d_col, dim=0, dim_size=data.x.shape[0], reduce=\"sum\")+scatter(neg_assoc,d_row, dim=0, dim_size=data.x.shape[0], reduce=\"sum\")\n",
    "        \n",
    "\n",
    "        x = torch.cat([posdeg.unsqueeze(-1), negdeg.unsqueeze(-1)],dim=1)*1.\n",
    "        x = x.float()\n",
    "\n",
    "        ##UNSQUEEZED\n",
    "        #x = F.leaky_relu(self.conv1(x.unsqueeze(-1), edge_index))# +x\n",
    "        adjusted_ea = torch.repeat_interleave(edge_attr,self.hidden2,1)\n",
    "        adjusted_ea = adjusted_ea.float()\n",
    "        x = x.squeeze(-1)\n",
    "        x = x.float()\n",
    "        \n",
    "        iterations = 0\n",
    "        total_loss = []\n",
    "        while (iterations < self.num_iterations):\n",
    "            \n",
    "            if(iterations>0):\n",
    "                x = torch.cat([posdeg.unsqueeze(-1), negdeg.unsqueeze(-1)],dim=1)*1.\n",
    "                x = x.float()\n",
    "        \n",
    "                recurr =  torch.repeat_interleave(transformed_probs_comp.unsqueeze(-1),self.hidden2,1)\n",
    "                recurr_activ =  F.leaky_relu(self.conv_alt_rec(recurr, edge_index,adjusted_ea))\n",
    "                x = F.leaky_relu(self.conv_alt(x, edge_index,adjusted_ea))  * recurr_activ\n",
    "                                 \n",
    "            else:\n",
    "                #FIRST CONVO\n",
    "                x = F.leaky_relu(self.conv1(x, edge_index,adjusted_ea))# +x\n",
    "\n",
    "            x_1 = x.clone()\n",
    "            \n",
    "            #ADJUST EDGE \n",
    "            adjusted_ea_2 = torch.repeat_interleave(edge_attr.unsqueeze(-1), self.hidden1,1)\n",
    "            adjusted_ea_2 = (adjusted_ea_2.squeeze(-1)).float()\n",
    "\n",
    "\n",
    "            x =  x+F.leaky_relu(self.conv(x, edge_index,edge_attr.float())) #+ x_1 GRU MODE\n",
    "\n",
    "            x =  F.leaky_relu(self.lin1(x)) \n",
    "            x = F.leaky_relu(self.lin2(x)) \n",
    "\n",
    "            probs = torch.sigmoid(x)*0.999990 + 0.000001 \n",
    "            probs = probs.squeeze(-1) #shape: x\n",
    "            probs_backup = probs.clone()\n",
    "\n",
    "            #sanity check\n",
    "            #WARNING EDGE ATTR!= DATA.EDGE_ATTR\n",
    "            pos_neg_index = (data.edge_attr==-1)*1.\n",
    "            test_probs = torch.ones_like(probs)*0.25\n",
    "            test_probs = test_probs.squeeze(-1)\n",
    "            test_msg = (1-pos_neg_index) *(1- test_probs[d_row]) + (pos_neg_index)*(test_probs[d_row])\n",
    "            test_msg_c = (pos_neg_index) *(1- test_probs[d_row]) + (1-pos_neg_index)*(test_probs[d_row])\n",
    "\n",
    "            msg = pos_neg_index *(1- probs[d_row]) + (1-pos_neg_index)*(probs[d_row])\n",
    "            comp_msg = pos_neg_index *( probs[d_row]) + (1-pos_neg_index)*(1-probs[d_row])\n",
    "            pre_exp_comp = scatter(torch.log(comp_msg),d_col,dim=0, reduce=\"sum\")\n",
    "            pre_exp = scatter(torch.log(msg),d_col,dim=0, reduce=\"sum\")\n",
    "            transformed_probs = torch.exp(pre_exp)\n",
    "            transformed_probs_comp = torch.exp(pre_exp_comp)\n",
    "            clause_probs = transformed_probs.squeeze(-1)*((x_index==1)*1.)\n",
    "            clause_probs_comp = transformed_probs_comp.squeeze(-1)*((x_index==1)*1.) #shape: x\n",
    "            var_probs = probs[data.x==0].detach()\n",
    "\n",
    "            loss_comp = scatter(clause_probs, batch, dim=0 , reduce=\"sum\") #expected_loss#*normalize\n",
    "            loss = scatter(clause_probs_comp, batch, dim=0 , reduce=\"sum\") #expected_loss#*normalize\n",
    "            \n",
    "            num_clauses = (map(len, [i[0].clauses for i in data.formula]))\n",
    "            uniform_prob = torch.tensor([0.125*i for i in num_clauses]) #union bound for uniform probability\n",
    "\n",
    "            total_loss += [loss.mean()]\n",
    "            \n",
    "            iterations += 1\n",
    "\n",
    "            \n",
    "        exponents = torch.arange(self.num_iterations,device='cuda')\n",
    "        \n",
    "                ## NUM CLAUSES, C/V RATIO, LLL SAT RATIO \n",
    "        num_vars = torch.tensor([(data.x[data.batch==num]==0).sum().item() for num in data.batch.unique()], device='cuda')\n",
    "        num_clauses =  torch.tensor([(data.x[data.batch==num]==1).sum().item() for num in data.batch.unique()], device='cuda')\n",
    "        LLL_check =  clause_probs_comp\n",
    "        LLL_diff = clause_probs_comp[select_clauses]\n",
    "        LLL_loss = scatter(LLL_diff, batch[select_clauses], dim=0 , reduce=\"sum\") #shape: 32\n",
    "        UnionBoundCheck = (LLL_loss<=1.)*1.\n",
    "        \n",
    "        #breakpoint()\n",
    "        #### RATIOS/STATS\n",
    "        c_v_ratio = num_clauses/num_vars\n",
    "        clauses_satisfied = scatter(LLL_check*1., batch, dim=0 , reduce=\"sum\")        \n",
    "        perc_of_clauses_satisfied_per_graph = clauses_satisfied/num_clauses\n",
    "\n",
    "\n",
    "        retdict = {}\n",
    "        ub_term = LLL_loss\n",
    "        final_loss = ub_term # + LLL_gamma*LLL_prob\n",
    "        \n",
    "        #################################\n",
    "        #scatter plot\n",
    "        hardness_unionbound_scatterplot = torch.cat([c_v_ratio.unsqueeze(-1), loss.unsqueeze(-1)],dim=1)\n",
    "\n",
    "\n",
    "        #color\n",
    "        color = (c_v_ratio.unsqueeze(-1)).detach()\n",
    "        color = color/color.max().item()\n",
    "        color = torch.round(color*255.)\n",
    "        dummy = torch.zeros_like(color)\n",
    "        color_r = torch.cat([color ,dummy,dummy],dim=1)\n",
    "        color_g = torch.cat([dummy,color, dummy],dim=1)\n",
    "        \n",
    "\n",
    "        is_sat = torch.tensor([sat*1. for sat in data.sat],device='cuda').unsqueeze(-1)\n",
    "        final_color = is_sat * color_g  + (1-is_sat)*color_r\n",
    "\n",
    "        retdict[\"output\"] = [probs.squeeze(-1),\"hist\"]   #output\n",
    "        retdict[\"union bound avg\"] = [LLL_loss.mean().squeeze(),\"sequence\"] #final loss\n",
    "        retdict[\"loss\"] = [final_loss.mean().squeeze(),\"sequence\"] #final loss\n",
    "        retdict[\"union bound\"] = [loss.mean().squeeze(),\"sequence\"] #final loss\n",
    "        retdict[\"loss_comp\"] = [loss_comp.mean().squeeze(),\"null\"] #final loss\n",
    "        # retdict[\"percent LLL constraints satisfied\"] =  [true_perc_of_clauses_satisfied_per_graph,'hist']\n",
    "        # retdict[\"average percentage of LLL constraints satisfied\"] =  [true_perc_of_clauses_satisfied_per_graph.mean(),'sequence']\n",
    "        retdict[\"union bound/hardness (c/v ratio)\"] = [hardness_unionbound_scatterplot, 'scatter',final_color]\n",
    "        #retdict[\"LLL bound sat percentage/hardness (c/v ratio)\"] = [hardness_lllbound_scatterplot, 'scatter',final_color]\n",
    "        retdict[\"clause_probs\"] = [clause_probs_comp[data.x==1], \"hist\"]\n",
    "        retdict[\"clause_probs_full\"] = [clause_probs_comp, \"null\"]\n",
    "        retdict['variable probs'] = [var_probs.squeeze(),'hist']\n",
    "        retdict[\"variable_probs_full\"] = [probs,'null']\n",
    "        retdict[\"union bounds\"] = [LLL_loss.squeeze(),'hist']\n",
    "        retdict[\"union bound sat\"] = [UnionBoundCheck.squeeze(),'hist']\n",
    "\n",
    "        \n",
    "        return retdict\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "numlayers=2\n",
    "receptive_field= numlayers + 1\n",
    "val_losses = []\n",
    "cliq_dists = []\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "lr_decay_step_size = 20\n",
    "lr_decay_factor = 0.95\n",
    "lr_lower_bound = 0.00001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir 'ErdoSAT_repo'\n",
    "#for validation checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current graph:  0\n",
      "current graph:  1\n",
      "current graph:  2\n",
      "current graph:  3\n",
      "current graph:  4\n",
      "current graph:  5\n",
      "current graph:  6\n",
      "current graph:  7\n",
      "current graph:  8\n",
      "current graph:  9\n",
      "current graph:  10\n",
      "current graph:  11\n",
      "current graph:  12\n",
      "current graph:  13\n",
      "current graph:  14\n",
      "current graph:  15\n",
      "current graph:  16\n",
      "current graph:  17\n",
      "current graph:  18\n",
      "current graph:  19\n",
      "current graph:  20\n",
      "current graph:  21\n",
      "current graph:  22\n",
      "current graph:  23\n",
      "current graph:  24\n",
      "current graph:  25\n",
      "current graph:  26\n",
      "current graph:  27\n",
      "current graph:  28\n",
      "current graph:  29\n",
      "current graph:  30\n",
      "current graph:  31\n",
      "current graph:  32\n",
      "current graph:  33\n",
      "current graph:  34\n",
      "current graph:  35\n",
      "current graph:  36\n",
      "current graph:  37\n",
      "current graph:  38\n",
      "current graph:  39\n",
      "current graph:  40\n",
      "current graph:  41\n",
      "current graph:  42\n",
      "current graph:  43\n",
      "current graph:  44\n",
      "current graph:  45\n",
      "current graph:  46\n",
      "current graph:  47\n",
      "current graph:  48\n",
      "current graph:  49\n",
      "current graph:  50\n",
      "current graph:  51\n",
      "current graph:  52\n",
      "current graph:  53\n",
      "current graph:  54\n",
      "current graph:  55\n",
      "current graph:  56\n",
      "current graph:  57\n",
      "current graph:  58\n",
      "current graph:  59\n",
      "current graph:  60\n",
      "current graph:  61\n",
      "current graph:  62\n",
      "current graph:  63\n",
      "current graph:  64\n",
      "current graph:  65\n",
      "current graph:  66\n",
      "current graph:  67\n",
      "current graph:  68\n",
      "current graph:  69\n",
      "current graph:  70\n",
      "current graph:  71\n",
      "current graph:  72\n",
      "current graph:  73\n",
      "current graph:  74\n",
      "current graph:  75\n",
      "current graph:  76\n",
      "current graph:  77\n",
      "current graph:  78\n",
      "current graph:  79\n",
      "current graph:  80\n",
      "current graph:  81\n",
      "current graph:  82\n",
      "current graph:  83\n",
      "current graph:  84\n",
      "current graph:  85\n",
      "current graph:  86\n",
      "current graph:  87\n",
      "current graph:  88\n",
      "current graph:  89\n",
      "current graph:  90\n",
      "current graph:  91\n",
      "current graph:  92\n",
      "current graph:  93\n",
      "current graph:  94\n",
      "current graph:  95\n",
      "current graph:  96\n",
      "current graph:  97\n",
      "current graph:  98\n",
      "current graph:  99\n",
      "current graph:  100\n",
      "current graph:  101\n",
      "current graph:  102\n",
      "current graph:  103\n",
      "current graph:  104\n",
      "current graph:  105\n",
      "current graph:  106\n",
      "current graph:  107\n",
      "current graph:  108\n",
      "current graph:  109\n",
      "current graph:  110\n",
      "current graph:  111\n",
      "current graph:  112\n",
      "current graph:  113\n",
      "current graph:  114\n",
      "current graph:  115\n",
      "current graph:  116\n",
      "current graph:  117\n",
      "current graph:  118\n",
      "current graph:  119\n",
      "current graph:  120\n",
      "current graph:  121\n",
      "current graph:  122\n",
      "current graph:  123\n",
      "current graph:  124\n",
      "current graph:  125\n",
      "current graph:  126\n",
      "current graph:  127\n",
      "current graph:  128\n",
      "current graph:  129\n",
      "current graph:  130\n",
      "current graph:  131\n",
      "current graph:  132\n",
      "current graph:  133\n",
      "current graph:  134\n",
      "current graph:  135\n",
      "current graph:  136\n",
      "current graph:  137\n",
      "current graph:  138\n",
      "current graph:  139\n",
      "current graph:  140\n",
      "current graph:  141\n",
      "current graph:  142\n",
      "current graph:  143\n",
      "current graph:  144\n",
      "current graph:  145\n",
      "current graph:  146\n",
      "current graph:  147\n",
      "current graph:  148\n",
      "current graph:  149\n",
      "current graph:  150\n",
      "current graph:  151\n",
      "current graph:  152\n",
      "current graph:  153\n",
      "current graph:  154\n",
      "current graph:  155\n",
      "current graph:  156\n",
      "current graph:  157\n",
      "current graph:  158\n",
      "current graph:  159\n",
      "current graph:  160\n",
      "current graph:  161\n",
      "current graph:  162\n",
      "current graph:  163\n",
      "current graph:  164\n",
      "current graph:  165\n",
      "current graph:  166\n",
      "current graph:  167\n",
      "current graph:  168\n",
      "current graph:  169\n",
      "current graph:  170\n",
      "current graph:  171\n",
      "current graph:  172\n",
      "current graph:  173\n",
      "current graph:  174\n",
      "current graph:  175\n",
      "current graph:  176\n",
      "current graph:  177\n",
      "current graph:  178\n",
      "current graph:  179\n",
      "current graph:  180\n",
      "current graph:  181\n",
      "current graph:  182\n",
      "current graph:  183\n",
      "current graph:  184\n",
      "current graph:  185\n",
      "current graph:  186\n",
      "current graph:  187\n",
      "current graph:  188\n",
      "current graph:  189\n",
      "current graph:  190\n",
      "current graph:  191\n",
      "current graph:  192\n",
      "current graph:  193\n",
      "current graph:  194\n",
      "current graph:  195\n",
      "current graph:  196\n",
      "current graph:  197\n",
      "current graph:  198\n",
      "current graph:  199\n",
      "current graph:  200\n",
      "current graph:  201\n",
      "current graph:  202\n",
      "current graph:  203\n",
      "current graph:  204\n",
      "current graph:  205\n",
      "current graph:  206\n",
      "current graph:  207\n",
      "current graph:  208\n",
      "current graph:  209\n",
      "current graph:  210\n",
      "current graph:  211\n",
      "current graph:  212\n",
      "current graph:  213\n",
      "current graph:  214\n",
      "current graph:  215\n",
      "current graph:  216\n",
      "current graph:  217\n",
      "current graph:  218\n",
      "current graph:  219\n",
      "current graph:  220\n",
      "current graph:  221\n",
      "current graph:  222\n",
      "current graph:  223\n",
      "current graph:  224\n",
      "current graph:  225\n",
      "current graph:  226\n",
      "current graph:  227\n",
      "current graph:  228\n",
      "current graph:  229\n",
      "current graph:  230\n",
      "current graph:  231\n",
      "current graph:  232\n",
      "current graph:  233\n",
      "current graph:  234\n",
      "current graph:  235\n",
      "current graph:  236\n",
      "current graph:  237\n",
      "current graph:  238\n",
      "current graph:  239\n",
      "current graph:  240\n",
      "current graph:  241\n",
      "current graph:  242\n",
      "current graph:  243\n",
      "current graph:  244\n",
      "current graph:  245\n",
      "current graph:  246\n",
      "current graph:  247\n",
      "current graph:  248\n",
      "current graph:  249\n",
      "current graph:  250\n",
      "current graph:  251\n",
      "current graph:  252\n",
      "current graph:  253\n",
      "current graph:  254\n",
      "current graph:  255\n",
      "current graph:  256\n",
      "current graph:  257\n",
      "current graph:  258\n",
      "current graph:  259\n",
      "current graph:  260\n",
      "current graph:  261\n",
      "current graph:  262\n",
      "current graph:  263\n",
      "current graph:  264\n",
      "current graph:  265\n",
      "current graph:  266\n",
      "current graph:  267\n",
      "current graph:  268\n",
      "current graph:  269\n",
      "current graph:  270\n",
      "current graph:  271\n",
      "current graph:  272\n",
      "current graph:  273\n",
      "current graph:  274\n",
      "current graph:  275\n",
      "current graph:  276\n",
      "current graph:  277\n",
      "current graph:  278\n",
      "current graph:  279\n",
      "current graph:  280\n",
      "current graph:  281\n",
      "current graph:  282\n",
      "current graph:  283\n",
      "current graph:  284\n",
      "current graph:  285\n",
      "current graph:  286\n",
      "current graph:  287\n",
      "current graph:  288\n",
      "current graph:  289\n",
      "current graph:  290\n",
      "current graph:  291\n",
      "current graph:  292\n",
      "current graph:  293\n",
      "current graph:  294\n",
      "current graph:  295\n",
      "current graph:  296\n",
      "current graph:  297\n",
      "current graph:  298\n",
      "current graph:  299\n"
     ]
    }
   ],
   "source": [
    "##validation set\n",
    "big_large_easy_validation = generate_RandomCNFDataset(300,3,100,400,num_variables_high=101, num_clauses_high=401);\n",
    "infinite_data = True\n",
    "if not infinite_data:\n",
    "    big_large_easy_train = generate_RandomCNFDataset(1500,3,100,400,num_variables_high=101, num_clauses_high=401);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aspect/anaconda3/envs/extensions/lib/python3.9/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Validation Loss:  tensor(48.7261, device='cuda:0')\n",
      "SAVED!\n",
      "Epoch:  0\n",
      "Average Epoch Loss:  tensor(51.6350, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch:  1\n",
      "Average Epoch Loss:  tensor(51.7864, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch:  2\n",
      "Average Epoch Loss:  tensor(41.4148, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch:  3\n",
      "Average Epoch Loss:  tensor(34.8449, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch:  4\n",
      "Average Epoch Loss:  tensor(32.1847, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch:  5\n",
      "Average Epoch Loss:  tensor(31.1938, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch:  6\n",
      "Average Epoch Loss:  tensor(30.2098, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch:  7\n",
      "Average Epoch Loss:  tensor(29.7994, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch:  8\n",
      "Average Epoch Loss:  tensor(29.0614, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch:  9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 98\u001b[0m\n\u001b[1;32m     96\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(), \n\u001b[1;32m     97\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 98\u001b[0m retdict \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m loss \u001b[38;5;241m=\u001b[39m retdict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    100\u001b[0m avg_epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "File \u001b[0;32m~/anaconda3/envs/extensions/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/extensions/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 187\u001b[0m, in \u001b[0;36mErdos_MPNN.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    184\u001b[0m exponents \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_iterations,device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;66;03m## NUM CLAUSES, C/V RATIO, LLL SAT RATIO \u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m num_vars \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([(data\u001b[38;5;241m.\u001b[39mx[data\u001b[38;5;241m.\u001b[39mbatch\u001b[38;5;241m==\u001b[39mnum]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m num \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mbatch\u001b[38;5;241m.\u001b[39munique()], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    188\u001b[0m num_clauses \u001b[38;5;241m=\u001b[39m  torch\u001b[38;5;241m.\u001b[39mtensor([(data\u001b[38;5;241m.\u001b[39mx[data\u001b[38;5;241m.\u001b[39mbatch\u001b[38;5;241m==\u001b[39mnum]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m num \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mbatch\u001b[38;5;241m.\u001b[39munique()], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    189\u001b[0m LLL_check \u001b[38;5;241m=\u001b[39m  clause_probs_comp\n",
      "Cell \u001b[0;32mIn[10], line 187\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    184\u001b[0m exponents \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_iterations,device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;66;03m## NUM CLAUSES, C/V RATIO, LLL SAT RATIO \u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m num_vars \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([(\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43mnum\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m num \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mbatch\u001b[38;5;241m.\u001b[39munique()], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    188\u001b[0m num_clauses \u001b[38;5;241m=\u001b[39m  torch\u001b[38;5;241m.\u001b[39mtensor([(data\u001b[38;5;241m.\u001b[39mx[data\u001b[38;5;241m.\u001b[39mbatch\u001b[38;5;241m==\u001b[39mnum]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m num \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mbatch\u001b[38;5;241m.\u001b[39munique()], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    189\u001b[0m LLL_check \u001b[38;5;241m=\u001b[39m  clause_probs_comp\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#####################TODOS:\n",
    "b_sizes = [16]\n",
    "l_rates = [0.001]\n",
    "depths = [10]\n",
    "#rand_seeds = list(range(70,80))\n",
    "rand_seeds = [3]\n",
    "widths = [128]\n",
    "\n",
    "num_iterations_per_epoch = 10\n",
    "\n",
    "wandb_logging = False  \n",
    "rnn_its = 1\n",
    "epochs = 1000\n",
    "retdict = {}\n",
    "edge_drop_p = 0.0\n",
    "edge_dropout_decay = 0.90\n",
    "validation_timeout = 75\n",
    "vis_batch = None\n",
    "vis_batches = []\n",
    "performance_monitor = []\n",
    "curr_date = date.today()\n",
    "curr_date = curr_date.strftime(\"%B %d, %Y\")\n",
    "curr_LR = None\n",
    "for batch_size, learning_rate, numlayers,  r_seed, hidden_1 in product(b_sizes, l_rates, depths, rand_seeds, widths):\n",
    "\n",
    "    torch.manual_seed(r_seed)\n",
    "    valdata = big_large_easy_validation\n",
    "    val_loader =  DataLoader(valdata, batch_size, shuffle=False)\n",
    "\n",
    "#####Set up data and loaders \n",
    "    if not infinite_data:\n",
    "        traindata= big_large_easy_train\n",
    "        train_loader = DataLoader(traindata, batch_size, shuffle=True)\n",
    "\n",
    "#####Set up model and relevant vars\n",
    "    #num_sat_instances =  sum([1*data.sat for data in traindata])\n",
    "    val_losses = []\n",
    "    cliq_dists = []\n",
    "    #hidden_1 = 128\n",
    "    hidden_2 = 2\n",
    "    net =  Erdos_MPNN(num_layers = numlayers, hidden1= hidden_1, hidden2 = hidden_2 ,num_iterations=rnn_its)\n",
    "    net.to(device).reset_parameters()\n",
    "    optimizer = Adam(net.parameters(), lr=learning_rate, weight_decay=0.00000)\n",
    "    net.train()\n",
    "    graphs_solved_LLL = 0\n",
    "    graphs_solved_LLL_previous = 0\n",
    "    #with torch.autograd.set_detect_anomaly(True):\n",
    "    for epoch in range(epochs):\n",
    "        count = 0\n",
    "        avg_epoch_loss = 0\n",
    "        totalretdict = {}\n",
    "        #learning rate schedule\n",
    "        if epoch % lr_decay_step_size == 0:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                        new_lr = lr_decay_factor * param_group['lr']\n",
    "                        if new_lr > lr_lower_bound:\n",
    "                            param_group['lr'] = new_lr\n",
    "                        else:\n",
    "                            param_group['lr'] = lr_lower_bound\n",
    "                        if (param_group['lr'] <= lr_lower_bound) and (performance_monitor[-1] <= 0.999*performance_monitor[-50]):\n",
    "                            param_group['lr'] = param_group['lr']*1\n",
    "\n",
    "                        curr_LR =  param_group['lr']\n",
    "                \n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                net.eval()\n",
    "                for data in val_loader:\n",
    "                    count += 1 \n",
    "                    optimizer.zero_grad(), \n",
    "                    data = data.to(device)\n",
    "                    retdict = net(data)\n",
    "                    val_loss += retdict[\"loss\"][0]/len(val_loader)\n",
    "                    optimizer.zero_grad()\n",
    "        \n",
    "            print(\"Current Validation Loss: \", val_loss)\n",
    "            val_losses += [val_loss]\n",
    "            #parent_path = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "            PATH = \"ErdoSAT_repo/\"+str(val_loss)+str(epoch)+\"ErdoSATmodel.pt\"\n",
    "            if ((val_loss<=torch.stack(val_losses))*1.).prod():\n",
    "                 print(\"SAVED!\")\n",
    "                 torch.save({'epoch': epoch,'model_state_dict': net.state_dict(),'optimizer_state_dict': optimizer.state_dict(),'loss': val_loss}, PATH)                 \n",
    "\n",
    "\n",
    "\n",
    "        ####Infinite data\n",
    "        if infinite_data:\n",
    "            traindata = generate_RandomCNFDataset(num_iterations_per_epoch*batch_size,3,100,400,num_variables_high=101, num_clauses_high=401,verbose=False);\n",
    "            train_loader = DataLoader(traindata, batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "        print('Epoch: ', epoch)\n",
    "        net.train()\n",
    "        for data in train_loader:\n",
    "            count += 1 \n",
    "            optimizer.zero_grad(), \n",
    "            data = data.to(device)\n",
    "            retdict = net(data)\n",
    "            loss = retdict[\"loss\"][0]\n",
    "            avg_epoch_loss += loss/len(train_loader)\n",
    "            for key,val in retdict.items():\n",
    "                if \"sequence\" in val[1]:\n",
    "                    if key in totalretdict:\n",
    "                        totalretdict[key][0] += val[0].item()\n",
    "                    else:\n",
    "                        totalretdict[key] = [val[0].item(),val[1]]\n",
    "            if epoch > 1:\n",
    "                    loss.backward()\n",
    "                    #print(retdict[\"loss\"][0])\n",
    "                    #torch.nn.utils.clip_grad_norm_(net.parameters(),4.)\n",
    "                    optimizer.step()\n",
    "            else:\n",
    "                    optimizer.zero_grad()\n",
    "        print(\"Average Epoch Loss (avg unsat clauses): \", avg_epoch_loss)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lovasz_MPNN"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###LOAD CHECKPOINT (can skip that step)\n",
    "checkpoint = torch.load('ErdoSAT_repo/tensor(6.2546980ErdoSATmodel.pt')\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aspect/anaconda3/envs/extensions/lib/python3.9/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current batch:  0\n",
      "Current batch:  1\n",
      "Current batch:  2\n",
      "Current batch:  3\n",
      "Current batch:  4\n",
      "Current batch:  5\n",
      "Current batch:  6\n",
      "Current batch:  7\n",
      "Current batch:  8\n",
      "Current batch:  9\n",
      "Current batch:  10\n",
      "Current batch:  11\n",
      "Current batch:  12\n",
      "Current batch:  13\n",
      "Current batch:  14\n",
      "Current batch:  15\n",
      "Current batch:  16\n",
      "Current batch:  17\n",
      "Current batch:  18\n",
      "Current batch:  19\n",
      "Current batch:  20\n",
      "Current batch:  21\n",
      "Current batch:  22\n",
      "Current batch:  23\n",
      "Current batch:  24\n",
      "Current batch:  25\n",
      "Current batch:  26\n",
      "Current batch:  27\n",
      "Current batch:  28\n",
      "Current batch:  29\n",
      "Current batch:  30\n",
      "Current batch:  31\n",
      "Current batch:  32\n",
      "Current batch:  33\n",
      "Current batch:  34\n",
      "Current batch:  35\n",
      "Current batch:  36\n",
      "Current batch:  37\n",
      "Current batch:  38\n",
      "Current batch:  39\n",
      "Current batch:  40\n",
      "Current batch:  41\n",
      "Current batch:  42\n",
      "Current batch:  43\n",
      "Current batch:  44\n",
      "Current batch:  45\n",
      "Current batch:  46\n",
      "Current batch:  47\n",
      "Current batch:  48\n",
      "Current batch:  49\n",
      "Current batch:  50\n",
      "Current batch:  51\n",
      "Current batch:  52\n",
      "Current batch:  53\n",
      "Current batch:  54\n",
      "Current batch:  55\n",
      "Current batch:  56\n",
      "Current batch:  57\n",
      "Current batch:  58\n",
      "Current batch:  59\n",
      "Current batch:  60\n",
      "Current batch:  61\n",
      "Current batch:  62\n"
     ]
    }
   ],
   "source": [
    "dataset = test_dataset_torch\n",
    "test_loader = DataLoader(test_dataset_torch, batch_size, shuffle=False)\n",
    "expected_unsat_clauses_before =  []\n",
    "unsat_clauses_after = []\n",
    "\n",
    "batch_counter = 0\n",
    "\n",
    "for data in test_loader:\n",
    "    net.eval()\n",
    "    data = data.to(device)\n",
    "    retdict = net(data)\n",
    "    print(\"Current batch: \", batch_counter)\n",
    "\n",
    "    batch_probs = retdict['output'][0]\n",
    "    new_batch_probs = batch_probs.detach()\n",
    "\n",
    "    for graph_no,graph in enumerate(data.to_data_list()):\n",
    "        probs = batch_probs[data.batch==graph_no]\n",
    "        select_vars = (graph.x==0)\n",
    "        select_clauses = (graph.x==1)\n",
    "        d_row, d_col = graph.edge_index.detach()\n",
    "        edge_attr = graph.edge_attr.detach()\n",
    "        pos_assoc = edge_attr*((edge_attr>0)*1.)\n",
    "        neg_assoc = edge_attr*((edge_attr<0)*1.)\n",
    "        pos_neg_index = (graph.edge_attr==-1)*1.\n",
    "        new_probs = probs.detach()\n",
    "        new_var_probs = new_probs[select_vars]\n",
    "        \n",
    "\n",
    "\n",
    "        comp_msg = pos_neg_index *( new_probs[d_row]) + (1-pos_neg_index)*(1-new_probs[d_row])\n",
    "        pre_exp_comp = scatter(torch.log(comp_msg),d_col,dim=0, reduce=\"sum\")\n",
    "        transformed_probs_comp = torch.exp(pre_exp_comp)\n",
    "        clause_probs_comp = transformed_probs_comp.squeeze(-1)*((select_clauses)*1.) #shape: x\n",
    "        clause_probs_comp  = clause_probs_comp[select_clauses]\n",
    "        running_expected_clause_violation = clause_probs_comp.sum()\n",
    "        expected_unsat_clauses_before +=  [running_expected_clause_violation]\n",
    "\n",
    "\n",
    "        for counter in range(len(new_var_probs)):\n",
    "            if (new_var_probs[counter]<=1e-4):\n",
    "                new_var_probs[counter]=0.\n",
    "                continue\n",
    "            elif (new_var_probs[counter]>0.9999):\n",
    "                new_var_probs[counter]=1.\n",
    "                continue\n",
    "            p1 = new_var_probs[counter].detach()\n",
    "            one_minus_p1 = 1-p1\n",
    "            new_var_probs[counter] = 1.\n",
    "            #print(\"new probs value2\", new_var_probs[counter])\n",
    "\n",
    "            comp_msg = pos_neg_index *(new_var_probs[d_row]) + (1-pos_neg_index)*(1-new_var_probs[d_row])\n",
    "            pre_exp_comp = scatter(torch.log(comp_msg),d_col,dim=0, reduce=\"sum\")\n",
    "            transformed_probs_comp = torch.exp(pre_exp_comp)\n",
    "            clause_probs_comp = transformed_probs_comp.squeeze(-1)*((select_clauses)*1.) #shape: x\n",
    "            clause_probs_comp  = clause_probs_comp[select_clauses]\n",
    "            expected_clause_violation_1 = clause_probs_comp.sum()\n",
    "\n",
    "            \n",
    "            if expected_clause_violation_1<=running_expected_clause_violation:\n",
    "                #print(\"I'm here1\")\n",
    "                new_var_probs[counter] = 1.\n",
    "                running_expected_clause_violation = expected_clause_violation_1\n",
    "            else:\n",
    "                #print(\"ACTUALLY\")\n",
    "                new_var_probs[counter] = 0.\n",
    "                comp_msg = pos_neg_index *(new_var_probs[d_row]) + (1-pos_neg_index)*(1-new_var_probs[d_row])\n",
    "                pre_exp_comp = scatter(torch.log(comp_msg),d_col,dim=0, reduce=\"sum\")\n",
    "                transformed_probs_comp = torch.exp(pre_exp_comp)\n",
    "                clause_probs_comp = transformed_probs_comp.squeeze(-1)*((select_clauses)*1.) #shape: x\n",
    "                clause_probs_comp  = clause_probs_comp[select_clauses]\n",
    "                expected_clause_violation_0 = clause_probs_comp.sum()\n",
    "               # expected_clause_violation_0 = (running_expected_clause_violation - p1*expected_clause_violation_1)/one_minus_p1\n",
    "                running_expected_clause_violation = expected_clause_violation_0\n",
    "            #print(\"new probs value3\", new_var_probs[counter])\n",
    "\n",
    "\n",
    "        unsat_clauses_after +=[running_expected_clause_violation]\n",
    "            #var_probs = probs[select_vars].detach()\n",
    "        new_batch_probs[data.batch==graph_no] = new_probs\n",
    "\n",
    "    batch_counter +=1\n",
    "    #calculate for 1\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##SANITY CHECK: print 1 if scores after decoding must be equal or better than before decoding (expected behavior)\n",
    "((torch.stack(expected_unsat_clauses_before)>=torch.stack(unsat_clauses_after))*1.).prod()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unsat clauses is 6.079092502593994+/-2.0700221061706543\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of unsat clauses is {torch.stack(unsat_clauses_after).mean()}+/-{torch.stack(unsat_clauses_after).std()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "0e05d59f657a364f03308f37a9206f937286d15320c19782c29632429d4e08a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
